{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callback.noisy_student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy student\n",
    "\n",
    "> Callback to apply noisy student self-training (a semi-supervised learning approach) based on: Xie, Q., Luong, M. T., Hovy, E., & Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10687-10698)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from tsai.imports import *\n",
    "from tsai.utils import *\n",
    "from tsai.data.preprocessing import *\n",
    "from tsai.data.transforms import *\n",
    "from tsai.models.layers import *\n",
    "from fastai.callback.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# This is an unofficial implementation of noisy student based on:\n",
    "# Xie, Q., Luong, M. T., Hovy, E., & Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. \n",
    "# In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10687-10698).\n",
    "# Official tensorflow implementation available in https://github.com/google-research/noisystudent\n",
    "\n",
    "\n",
    "class NoisyStudent(Callback):\n",
    "    \"\"\"A callback to implement the Noisy Student approach. In the original paper this was used in combination with noise: \n",
    "        - stochastic depth: .8\n",
    "        - RandAugment: N=2, M=27\n",
    "        - dropout: .5\n",
    "        \n",
    "    Steps:\n",
    "        1. Build the dl you will use as a teacher\n",
    "        2. Create dl2 with the pseudolabels (either soft or hard preds)\n",
    "        3. Pass any required batch_tfms to the callback\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dl2:DataLoader, bs:Optional[int]=None, l2pl_ratio:int=1, batch_tfms:Optional[list]=None, do_setup:bool=True, \n",
    "                 pseudolabel_sample_weight:float=1., verbose=False): \n",
    "        r'''\n",
    "        Args:\n",
    "            dl2:                       dataloader with the pseudolabels\n",
    "            bs:                        batch size of the new, combined dataloader. If None, it will pick the bs from the labeled dataloader.\n",
    "            l2pl_ratio:                ratio between labels and pseudolabels in the combined batch\n",
    "            batch_tfms:                transforms applied to the combined batch. If None, it will pick the batch_tfms from the labeled dataloader (if any)\n",
    "            do_setup:                  perform a transform setup on the labeled dataset.\n",
    "            pseudolabel_sample_weight: weight of each pseudolabel sample relative to the labeled one of the loss.\n",
    "        '''\n",
    "        \n",
    "        self.dl2, self.bs, self.l2pl_ratio, self.batch_tfms, self.do_setup, self.verbose = dl2, bs, l2pl_ratio, batch_tfms, do_setup, verbose\n",
    "        self.pl_sw = pseudolabel_sample_weight\n",
    "        \n",
    "    def before_fit(self):\n",
    "        if self.batch_tfms is None: self.batch_tfms = self.dls.train.after_batch\n",
    "        self.old_bt = self.dls.train.after_batch # Remove and store dl.train.batch_tfms\n",
    "        self.old_bs = self.dls.train.bs\n",
    "        self.dls.train.after_batch = noop        \n",
    "\n",
    "        if self.do_setup and self.batch_tfms:\n",
    "            for bt in self.batch_tfms: \n",
    "                bt.setup(self.dls.train)\n",
    "\n",
    "        if self.bs is None: self.bs = self.dls.train.bs\n",
    "        self.dl2.bs = min(len(self.dl2.dataset), int(self.bs / (1 + self.l2pl_ratio)))\n",
    "        self.dls.train.bs = self.bs - self.dl2.bs\n",
    "        pv(f'labels / pseudolabels per training batch              : {self.dls.train.bs} / {self.dl2.bs}', self.verbose)\n",
    "        rel_weight = (self.dls.train.bs/self.dl2.bs) * (len(self.dl2.dataset)/len(self.dls.train.dataset))\n",
    "        pv(f'relative labeled/ pseudolabel sample weight in dataset: {rel_weight:.1f}', self.verbose)\n",
    "        self.dl2iter = iter(self.dl2)\n",
    "    \n",
    "        self.old_loss_func = self.learn.loss_func\n",
    "        self.learn.loss_func = self.loss\n",
    "        \n",
    "    def before_batch(self):\n",
    "        if self.training:\n",
    "            X, y = self.x, self.y\n",
    "            try: X2, y2 = next(self.dl2iter)\n",
    "            except StopIteration:\n",
    "                self.dl2iter = iter(self.dl2)\n",
    "                X2, y2 = next(self.dl2iter)\n",
    "            if y.ndim == 1 and y2.ndim == 2: y = torch.eye(self.learn.dls.c)[y].to(device) # ensure both \n",
    "            \n",
    "            X_comb, y_comb = concat(X, X2), concat(y, y2)\n",
    "            \n",
    "            if self.batch_tfms is not None: \n",
    "                X_comb = compose_tfms(X_comb, self.batch_tfms, split_idx=0)\n",
    "                y_comb = compose_tfms(y_comb, self.batch_tfms, split_idx=0)\n",
    "            self.learn.xb = (X_comb,)\n",
    "            self.learn.yb = (y_comb,)\n",
    "            pv(f'\\nX: {X.shape}  X2: {X2.shape}  X_comb: {X_comb.shape}', self.verbose)\n",
    "            pv(f'y: {y.shape}  y2: {y2.shape}  y_comb: {y_comb.shape}', self.verbose)\n",
    "            \n",
    "    def loss(self, output, target): \n",
    "        if target.ndim == 2: _, target = target.max(dim=1)\n",
    "        if self.training and self.pl_sw != 1: \n",
    "            loss = (1 - self.pl_sw) * self.old_loss_func(output[:self.dls.train.bs], target[:self.dls.train.bs])\n",
    "            loss += self.pl_sw * self.old_loss_func(output[self.dls.train.bs:], target[self.dls.train.bs:])\n",
    "            return loss \n",
    "        else: \n",
    "            return self.old_loss_func(output, target)\n",
    "    \n",
    "    def after_fit(self):\n",
    "        self.dls.train.after_batch = self.old_bt\n",
    "        self.learn.loss_func = self.old_loss_func\n",
    "        self.dls.train.bs = self.old_bs\n",
    "        self.dls.bs = self.old_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.data.all import *\n",
    "from tsai.models.all import *\n",
    "from tsai.tslearner import *\n",
    "dsid = 'NATOPS'\n",
    "X, y, splits = get_UCR_data(dsid, return_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels / pseudolabels per training batch              : 171 / 85\n",
      "relative labeled/ pseudolabel sample weight in dataset: 4.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.823240</td>\n",
       "      <td>1.780566</td>\n",
       "      <td>0.188889</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: torch.Size([171, 24, 51])  X2: torch.Size([85, 24, 51])  X_comb: torch.Size([256, 24, 52])\n",
      "y: torch.Size([171])  y2: torch.Size([85])  y_comb: torch.Size([256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nacho/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "pseudolabeled_data = X\n",
    "soft_preds = True\n",
    "\n",
    "pseudolabels = ToNumpyCategory()(y) if soft_preds else OneHot()(y)\n",
    "dsets2 = TSDatasets(pseudolabeled_data, pseudolabels)\n",
    "dl2 = TSDataLoader(dsets2, num_workers=0)\n",
    "noisy_student_cb = NoisyStudent(dl2, bs=256, l2pl_ratio=2, verbose=True)\n",
    "learn = TSClassifier(X, y, splits=splits, batch_tfms=[TSStandardize(), TSRandomSize(.5)], cbs=noisy_student_cb)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels / pseudolabels per training batch              : 171 / 85\n",
      "relative labeled/ pseudolabel sample weight in dataset: 4.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.833863</td>\n",
       "      <td>1.783813</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: torch.Size([171, 24, 51])  X2: torch.Size([85, 24, 51])  X_comb: torch.Size([256, 24, 43])\n",
      "y: torch.Size([171, 6])  y2: torch.Size([85, 6])  y_comb: torch.Size([256, 6])\n"
     ]
    }
   ],
   "source": [
    "pseudolabeled_data = X\n",
    "soft_preds = False\n",
    "\n",
    "pseudolabels = ToNumpyCategory()(y) if soft_preds else OneHot()(y)\n",
    "dsets2 = TSDatasets(pseudolabeled_data, pseudolabels)\n",
    "dl2 = TSDataLoader(dsets2, num_workers=0)\n",
    "noisy_student_cb = NoisyStudent(dl2, bs=256, l2pl_ratio=2, verbose=True)\n",
    "learn = TSClassifier(X, y, splits=splits, batch_tfms=[TSStandardize(), TSRandomSize(.5)], cbs=noisy_student_cb)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "out = create_scripts(); beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
