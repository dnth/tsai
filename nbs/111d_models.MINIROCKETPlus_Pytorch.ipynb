{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.MINIROCKETPlus_Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINIROCKETPlus Pytorch\n",
    "\n",
    "> A Very Fast (Almost) Deterministic Transform for Time Series Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a modified Pytorch implementation of MiniRocket originally developed by Malcolm McLean and Ignacio Oguiza and based on:\n",
    "\n",
    "Dempster, A., Schmidt, D. F., & Webb, G. I. (2020). MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series Classification. arXiv preprint arXiv:2012.08791.\n",
    "\n",
    "Original paper: https://arxiv.org/abs/2012.08791\n",
    "\n",
    "Original code:  https://github.com/angus924/minirocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "\n",
    "\n",
    "class MiniRocketFeaturesPlus(nn.Module):\n",
    "    fitting = False\n",
    "\n",
    "    def __init__(self, c_in, seq_len, num_features=10_000, max_dilations_per_kernel=32, kernel_size=9, max_num_channels=9, max_num_kernels=84,\n",
    "                 add_lsaz=False):\n",
    "        super(MiniRocketFeaturesPlus, self).__init__()\n",
    "        self.c_in, self.seq_len = c_in, seq_len\n",
    "        self.kernel_size, self.max_num_channels, self.add_lsaz = kernel_size, max_num_channels, add_lsaz\n",
    "\n",
    "        # Kernels\n",
    "        indices, pos_values = self.get_indices(kernel_size, max_num_kernels)\n",
    "        self.num_kernels = len(indices)\n",
    "        kernels = (-torch.ones(self.num_kernels, 1, self.kernel_size)).scatter_(2, indices, pos_values)\n",
    "        self.indices = indices\n",
    "        self.kernels = nn.Parameter(kernels.repeat(c_in, 1, 1), requires_grad=False)\n",
    "        if add_lsaz:\n",
    "            num_features = num_features // 2\n",
    "        self.num_features = num_features // self.num_kernels * self.num_kernels\n",
    "        self.max_dilations_per_kernel = max_dilations_per_kernel\n",
    "\n",
    "        # Dilations\n",
    "        self.set_dilations(seq_len)\n",
    "\n",
    "        # Channel combinations (multivariate)\n",
    "        if c_in > 1:\n",
    "            self.set_channel_combinations(c_in, max_num_channels)\n",
    "\n",
    "        # Bias\n",
    "        for i in range(self.num_dilations):\n",
    "            self.register_buffer(f'biases_{i}', torch.empty(\n",
    "                (self.num_kernels, self.num_features_per_dilation[i])))\n",
    "        self.register_buffer('prefit', torch.BoolTensor([False]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        _features = []\n",
    "        for i, (dilation, padding) in enumerate(zip(self.dilations, self.padding)):\n",
    "            _padding1 = i % 2\n",
    "\n",
    "            # Convolution\n",
    "            C = F.conv1d(x, self.kernels, padding=padding,\n",
    "                         dilation=dilation, groups=self.c_in)\n",
    "            if self.c_in > 1:  # multivariate\n",
    "                C = C.reshape(x.shape[0], self.c_in, self.num_kernels, -1)\n",
    "                channel_combination = getattr(\n",
    "                    self, f'channel_combinations_{i}')\n",
    "                C = torch.mul(C, channel_combination)\n",
    "                C = C.sum(1)\n",
    "\n",
    "            # Bias\n",
    "            if not self.prefit or self.fitting:\n",
    "                num_features_this_dilation = self.num_features_per_dilation[i]\n",
    "                bias_this_dilation = self.get_bias(\n",
    "                    C, num_features_this_dilation)\n",
    "                setattr(self, f'biases_{i}', bias_this_dilation)\n",
    "                if self.fitting:\n",
    "                    if i < self.num_dilations - 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        self.prefit = torch.BoolTensor([True])\n",
    "                        return\n",
    "                elif i == self.num_dilations - 1:\n",
    "                    self.prefit = torch.BoolTensor([True])\n",
    "            else:\n",
    "                bias_this_dilation = getattr(self, f'biases_{i}')\n",
    "\n",
    "            # Features\n",
    "            _features.append(self.get_PPVs(\n",
    "                C[:, _padding1::2], bias_this_dilation[_padding1::2]))\n",
    "            _features.append(self.get_PPVs(\n",
    "                C[:, 1-_padding1::2, padding:-padding], bias_this_dilation[1-_padding1::2]))\n",
    "\n",
    "        return torch.cat(_features, dim=1)\n",
    "\n",
    "    def fit(self, X, chunksize=None):\n",
    "        num_samples = X.shape[0]\n",
    "        if chunksize is None:\n",
    "            chunksize = min(num_samples, self.num_dilations * self.num_kernels)\n",
    "        else: \n",
    "            chunksize = min(num_samples, chunksize)\n",
    "        idxs = np.random.choice(num_samples, chunksize, False)\n",
    "        self.fitting = True\n",
    "        if isinstance(X, np.ndarray): \n",
    "            self(torch.from_numpy(X[idxs]).to(self.kernels.device))\n",
    "        else:\n",
    "            self(X[idxs].to(self.kernels.device))\n",
    "        self.fitting = False\n",
    "\n",
    "    def get_PPVs(self, C, bias):\n",
    "        C = C.unsqueeze(-1)\n",
    "        bias = bias.view(1, bias.shape[0], 1, bias.shape[1])\n",
    "        a = (C > bias).float().mean(2).flatten(1)\n",
    "        if self.add_lsaz:\n",
    "            dif = (C - bias)\n",
    "            b = (F.relu(dif).sum(2) /\n",
    "                 torch.clamp_min(torch.abs(dif).sum(2), 1e-8)).flatten(1)\n",
    "            return torch.cat((a, b), dim=1)\n",
    "        else:\n",
    "            return a\n",
    "\n",
    "    def set_dilations(self, input_length):\n",
    "        num_features_per_kernel = self.num_features // self.num_kernels\n",
    "        true_max_dilations_per_kernel = min(\n",
    "            num_features_per_kernel, self.max_dilations_per_kernel)\n",
    "        multiplier = num_features_per_kernel / true_max_dilations_per_kernel\n",
    "        max_exponent = np.log2((input_length - 1) / (self.kernel_size - 1))\n",
    "        dilations, num_features_per_dilation = \\\n",
    "            np.unique(np.logspace(0, max_exponent, true_max_dilations_per_kernel, base=2).astype(\n",
    "                np.int32), return_counts=True)\n",
    "        num_features_per_dilation = (\n",
    "            num_features_per_dilation * multiplier).astype(np.int32)\n",
    "        remainder = num_features_per_kernel - num_features_per_dilation.sum()\n",
    "        i = 0\n",
    "        while remainder > 0:\n",
    "            num_features_per_dilation[i] += 1\n",
    "            remainder -= 1\n",
    "            i = (i + 1) % len(num_features_per_dilation)\n",
    "        self.num_features_per_dilation = num_features_per_dilation\n",
    "        self.num_dilations = len(dilations)\n",
    "        self.dilations = dilations\n",
    "        self.padding = []\n",
    "        for i, dilation in enumerate(dilations):\n",
    "            self.padding.append((((self.kernel_size - 1) * dilation) // 2))\n",
    "\n",
    "    def set_channel_combinations(self, num_channels, max_num_channels):\n",
    "        num_combinations = self.num_kernels * self.num_dilations\n",
    "        if max_num_channels:\n",
    "            max_num_channels = min(num_channels, max_num_channels)\n",
    "        else:\n",
    "            max_num_channels = num_channels\n",
    "        max_exponent_channels = np.log2(max_num_channels + 1)\n",
    "        num_channels_per_combination = (\n",
    "            2 ** np.random.uniform(0, max_exponent_channels, num_combinations)).astype(np.int32)\n",
    "        self.num_channels_per_combination = num_channels_per_combination\n",
    "        channel_combinations = torch.zeros(\n",
    "            (1, num_channels, num_combinations, 1))\n",
    "        for i in range(num_combinations):\n",
    "            channel_combinations[:, np.random.choice(\n",
    "                num_channels, num_channels_per_combination[i], False), i] = 1\n",
    "        channel_combinations = torch.split(\n",
    "            channel_combinations, self.num_kernels, 2)  # split by dilation\n",
    "        for i, channel_combination in enumerate(channel_combinations):\n",
    "            self.register_buffer(\n",
    "                f'channel_combinations_{i}', channel_combination)  # per dilation\n",
    "\n",
    "    def get_quantiles(self, n):\n",
    "        return torch.tensor([(_ * ((np.sqrt(5) + 1) / 2)) % 1 for _ in range(1, n + 1)]).float()\n",
    "\n",
    "    def get_bias(self, C, num_features_this_dilation):\n",
    "        isp = torch.randint(C.shape[0], (self.num_kernels,))\n",
    "        samples = C[isp].diagonal().T\n",
    "        biases = torch.quantile(samples, self.get_quantiles(\n",
    "            num_features_this_dilation).to(C.device), dim=1).T\n",
    "        return biases\n",
    "\n",
    "    def get_indices(self, kernel_size, max_num_kernels):\n",
    "        num_pos_values = math.ceil(kernel_size / 3)\n",
    "        num_neg_values = kernel_size - num_pos_values\n",
    "        pos_values = num_neg_values / num_pos_values\n",
    "        if kernel_size > 9:\n",
    "            random_kernels = [np.sort(np.random.choice(kernel_size, num_pos_values, False)).reshape(\n",
    "                1, -1) for _ in range(max_num_kernels)]\n",
    "            indices = torch.from_numpy(\n",
    "                np.concatenate(random_kernels, 0)).unsqueeze(1)\n",
    "        else:\n",
    "            indices = torch.LongTensor(list(itertools.combinations(\n",
    "                np.arange(kernel_size), num_pos_values))).unsqueeze(1)\n",
    "            if max_num_kernels and len(indices) > max_num_kernels:\n",
    "                indices = indices[np.sort(np.random.choice(\n",
    "                    len(indices), max_num_kernels, False))]\n",
    "        return indices, pos_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "    \n",
    "\n",
    "class MiniRocketPlus(nn.Sequential):\n",
    "\n",
    "    def __init__(self, c_in, c_out, seq_len, num_features=10_000, max_dilations_per_kernel=32, kernel_size=9, max_num_channels=None, max_num_kernels=84,\n",
    "                 bn=True, fc_dropout=0, add_lsaz=False):\n",
    "\n",
    "        # Backbone\n",
    "        backbone = MiniRocketFeaturesPlus(c_in, seq_len, num_features=num_features, max_dilations_per_kernel=max_dilations_per_kernel,\n",
    "                                          kernel_size=kernel_size, max_num_channels=max_num_channels, max_num_kernels=max_num_kernels,\n",
    "                                          add_lsaz=add_lsaz)\n",
    "        num_features = backbone.num_features * (1 + add_lsaz)\n",
    "\n",
    "        # Head\n",
    "        self.head_nf = num_features\n",
    "        layers = [Flatten()]\n",
    "        if bn:\n",
    "            layers += [nn.BatchNorm1d(num_features)]\n",
    "        if fc_dropout:\n",
    "            layers += [nn.Dropout(fc_dropout)]\n",
    "        linear = nn.Linear(num_features, c_out)\n",
    "        nn.init.constant_(linear.weight.data, 0)\n",
    "        nn.init.constant_(linear.bias.data, 0)\n",
    "        layers += [linear]\n",
    "        head = nn.Sequential(*layers)\n",
    "\n",
    "        super().__init__(OrderedDict([('backbone', backbone), ('head', head)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_minirocket_features(o, model, chunksize=1024, use_cuda=None, to_np=False):\n",
    "    \"\"\"Function used to split a large dataset into chunks, avoiding OOM error.\"\"\"\n",
    "    use = torch.cuda.is_available() if use_cuda is None else use_cuda\n",
    "    device = torch.device(torch.cuda.current_device()\n",
    "                          ) if use else torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    if isinstance(o, np.ndarray):\n",
    "        o = torch.from_numpy(o).to(device)\n",
    "    _features = []\n",
    "    for oi in torch.split(o, chunksize):\n",
    "        _features.append(model(oi))\n",
    "    features = torch.cat(_features).unsqueeze(-1)\n",
    "    if to_np:\n",
    "        return features.cpu().numpy()\n",
    "    else:\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MiniRocketHead(nn.Sequential):\n",
    "    def __init__(self, c_in, c_out, seq_len=1, bn=True, fc_dropout=0.):\n",
    "        layers = [nn.Flatten()]\n",
    "        if bn:\n",
    "            layers += [nn.BatchNorm1d(c_in)]\n",
    "        if fc_dropout:\n",
    "            layers += [nn.Dropout(fc_dropout)]\n",
    "        linear = nn.Linear(c_in, c_out)\n",
    "        nn.init.constant_(linear.weight.data, 0)\n",
    "        nn.init.constant_(linear.bias.data, 0)\n",
    "        layers += [linear]\n",
    "        head = nn.Sequential(*layers)\n",
    "        super().__init__(OrderedDict(\n",
    "            [('backbone', nn.Sequential()), ('head', head)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.531260</td>\n",
       "      <td>0.751452</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Offline feature calculation\n",
    "from fastai.torch_core import default_device\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.callback.tracker import ReduceLROnPlateau\n",
    "from tsai.data.all import *\n",
    "from tsai.learner import *\n",
    "\n",
    "dsid = 'ECGFiveDays'\n",
    "X, y, splits = get_UCR_data(dsid, split_data=False)\n",
    "mrf = MiniRocketFeaturesPlus(c_in=X.shape[1], seq_len=X.shape[2]).to(default_device())\n",
    "X_train = X[splits[0]]  # X_train may either be a np.ndarray or a torch.Tensor\n",
    "mrf.fit(X_train)\n",
    "X_tfm = get_minirocket_features(X, mrf).cpu().numpy()\n",
    "tfms = [None, TSClassification()]\n",
    "batch_tfms = TSStandardize(by_var=True)\n",
    "dls = get_ts_dls(X_tfm, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=256)\n",
    "learn = ts_learner(dls, MiniRocketHead, metrics=accuracy)\n",
    "learn.fit(1, 1e-4, cbs=ReduceLROnPlateau(factor=0.5, min_lr=1e-8, patience=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.664867</td>\n",
       "      <td>0.502904</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Online feature calculation\n",
    "from fastai.torch_core import default_device\n",
    "from tsai.data.all import *\n",
    "from tsai.learner import *\n",
    "dsid = 'ECGFiveDays'\n",
    "X, y, splits = get_UCR_data(dsid, split_data=False)\n",
    "tfms = [None, TSClassification()]\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=256)\n",
    "learn = ts_learner(dls, MiniRocketPlus, kernel_size=7, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import *\n",
    "from tsai.models.utils import build_ts_model\n",
    "bs, c_in, seq_len = 8, 3, 50\n",
    "c_out = 2\n",
    "xb = torch.randn(bs, c_in, seq_len)\n",
    "model = build_ts_model(MiniRocketPlus, c_in=c_in, c_out=c_out, seq_len=seq_len)\n",
    "test_eq(model.to(xb.device)(xb).shape, (bs, c_out))\n",
    "model = build_ts_model(MiniRocketPlus, c_in=c_in, c_out=c_out, seq_len=seq_len, add_lsaz=True)\n",
    "test_eq(model.to(xb.device)(xb).shape, (bs, c_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class InceptionRocketFeaturesPlus(nn.Module):\n",
    "    fitting = False\n",
    "\n",
    "    def __init__(self, c_in, seq_len, num_features=10_000, max_dilations_per_kernel=32, kernel_sizes=np.arange(3, 10, 2),\n",
    "                 max_num_channels=None, max_num_kernels=84, add_lsaz=True, same_n_feats_per_ks=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.minirocketfeatures = nn.ModuleList()\n",
    "        kernel_sizes = [ks for ks in kernel_sizes if ks < seq_len]\n",
    "        self.kernel_sizes, self.max_num_kernels = kernel_sizes, max_num_kernels\n",
    "        if same_n_feats_per_ks:\n",
    "            num_features_per_kernel_size = [num_features // len(kernel_sizes)] * len(kernel_sizes)\n",
    "        else:\n",
    "            num_features_per_kernel_size = self._get_n_feat_per_ks(num_features)\n",
    "\n",
    "        self.num_features = 0\n",
    "        for kernel_size, num_features_this_kernel_size in zip(kernel_sizes, num_features_per_kernel_size):\n",
    "            self.minirocketfeatures.append(MiniRocketFeaturesPlus(c_in, seq_len, num_features=num_features_this_kernel_size,\n",
    "                                                                  max_dilations_per_kernel=max_dilations_per_kernel,\n",
    "                                                                  kernel_size=kernel_size,\n",
    "                                                                  max_num_channels=max_num_channels,\n",
    "                                                                  max_num_kernels=min(\n",
    "                                                                      max_num_kernels, num_features_this_kernel_size),\n",
    "                                                                  add_lsaz=add_lsaz))\n",
    "            self.num_features += self.minirocketfeatures[-1].num_features * (1 + add_lsaz)\n",
    "\n",
    "    def fit(self, X, chunksize=None):\n",
    "        for m in self.minirocketfeatures:\n",
    "            m.fit(X, chunksize=chunksize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for m in self.minirocketfeatures:\n",
    "            features.append(m(x))\n",
    "        return torch.cat(features, dim=1)\n",
    "\n",
    "    def _get_n_comb(self, kernel_size):\n",
    "        if kernel_size > 9: return self.max_num_kernels\n",
    "        return np.min([self.max_num_kernels, len(list(itertools.combinations(np.arange(kernel_size),  math.ceil(kernel_size / 3))))])\n",
    "\n",
    "    def _get_n_feat_per_ks(self, num_features):\n",
    "        combs = np.array([self._get_n_comb(ks) for ks in self.kernel_sizes])\n",
    "        num_features_per_kernel = num_features // np.sum(combs)\n",
    "        num_features_per_kernel_size = num_features_per_kernel * combs\n",
    "        return num_features_per_kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class InceptionRocketPlus(nn.Sequential):\n",
    "\n",
    "    def __init__(self, c_in, c_out, seq_len, num_features=10_000, max_dilations_per_kernel=32, kernel_sizes=[3, 5, 7, 9],\n",
    "                 max_num_channels=None, max_num_kernels=84, same_n_feats_per_ks=False, add_lsaz=False, bn=True, fc_dropout=0):\n",
    "\n",
    "        # Backbone\n",
    "        backbone = InceptionRocketFeaturesPlus(c_in, seq_len, num_features=num_features, max_dilations_per_kernel=max_dilations_per_kernel,\n",
    "                                               kernel_sizes=kernel_sizes, max_num_channels=max_num_channels, max_num_kernels=max_num_kernels,\n",
    "                                               same_n_feats_per_ks=same_n_feats_per_ks, add_lsaz=add_lsaz)\n",
    "        num_features = backbone.num_features\n",
    "\n",
    "        # Head\n",
    "        self.head_nf = num_features\n",
    "        layers = [Flatten()]\n",
    "        if bn:\n",
    "            layers += [nn.BatchNorm1d(num_features)]\n",
    "        if fc_dropout:\n",
    "            layers += [nn.Dropout(fc_dropout)]\n",
    "        linear = nn.Linear(num_features, c_out)\n",
    "        nn.init.constant_(linear.weight.data, 0)\n",
    "        nn.init.constant_(linear.bias.data, 0)\n",
    "        layers += [linear]\n",
    "        head = nn.Sequential(*layers)\n",
    "\n",
    "        super().__init__(OrderedDict([('backbone', backbone), ('head', head)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import *\n",
    "from tsai.models.utils import build_ts_model\n",
    "bs, c_in, seq_len = 8, 3, 50\n",
    "c_out = 2\n",
    "xb = torch.randn(bs, c_in, seq_len)\n",
    "model = build_ts_model(InceptionRocketPlus, c_in=c_in, c_out=c_out, seq_len=seq_len)\n",
    "test_eq(model.to(xb.device)(xb).shape, (bs, c_out))\n",
    "model = build_ts_model(InceptionRocketPlus, c_in=c_in, c_out=c_out, seq_len=seq_len, add_lsaz=True)\n",
    "test_eq(model.to(xb.device)(xb).shape, (bs, c_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from tsai.imports import *\n",
    "out = create_scripts(); beep(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
