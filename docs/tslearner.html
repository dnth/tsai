---

title: TSLearners (TSClassifier, TSRegressor, TSForecaster)


keywords: fastai
sidebar: home_sidebar

summary: "New set of time series learners with a new sklearn-like API that simplifies the learner creation."
description: "New set of time series learners with a new sklearn-like API that simplifies the learner creation."
nb_path: "nbs/052b_tslearner.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/052b_tslearner.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TSClassifier-API">TSClassifier API<a class="anchor-link" href="#TSClassifier-API"> </a></h2><hr>
<p><strong>Commonly used arguments:</strong></p>
<ul>
<li><strong>X:</strong> array-like of shape (n_samples, n_steps) or (n_samples, n_features, n_steps) with the input time series samples. Internally, they will be converted to torch tensors.</li>
<li><strong>y:</strong> array-like of shape (n_samples), (n_samples, n_outputs) or (n_samples, n_features, n_outputs) with the target. Internally, they will be converted to torch tensors. Default=None. None is used for unlabeled datasets. </li>
<li><strong>splits:</strong> lists of indices used to split data between train and validation. Default=None. If no splits are passed, data will be split 80:20 between train and test without shuffling.</li>
<li><strong>tfms:</strong> item transforms that will be applied to each sample individually. Default:<code>[None, TSClassification()]</code> which is commonly used in most single label datasets. </li>
<li><strong>batch_tfms:</strong> transforms applied to each batch. Default=None. </li>
<li><strong>bs:</strong> batch size (if batch_size is provided then batch_size will override bs). An int or a list of ints can be passed. Default=<code>[64, 128]</code>. If a list of ints, the first one will be used for training, and the second for the valid (batch size can be larger as it doesn't require backpropagation which consumes more memory). </li>
<li><strong>arch:</strong> indicates which architecture will be used. Default: InceptionTimePlus.</li>
<li><strong>arch_config:</strong> keyword arguments passed to the selected architecture. Default={}.</li>
<li><strong>pretrained:</strong> indicates if pretrained model weights will be used. Default=False.</li>
<li><strong>weights_path:</strong> indicates the path to the pretrained weights in case they are used.</li>
<li><strong>loss_func:</strong> allows you to pass any loss function. Default=None (in which case CrossEntropyLossFlat() is applied).</li>
<li><strong>opt_func:</strong> allows you to pass an optimizer. Default=Adam.</li>
<li><strong>lr:</strong> learning rate. Default=0.001.</li>
<li><strong>metrics:</strong> list of metrics passed to the Learner. Default=accuracy.</li>
<li><strong>cbs:</strong> list of callbacks passed to the Learner. Default=None.</li>
<li><strong>wd:</strong> is the default weight decay used when training the model. Default=None.</li>
</ul>
<p><strong>Less frequently used arguments:</strong></p>
<ul>
<li><strong>sel_vars:</strong> used to select which of the features in multivariate datasets are used. Default=None means all features are used. If necessary a list-like of indices can be used (eg.<code>[0,3,5]</code>).</li>
<li><strong>sel_steps:</strong> used to select the steps used. Default=None means all steps are used. If necessary a list-like of indices can be used (eg. <code>slice(-50, None)</code> will select the last 50 steps from each time series).</li>
<li><strong>weights:</strong> indicates a sample weight per instance. Used to pass pass a probability to the train dataloader sampler. Samples with more weight will be selected more often during training. </li>
<li><strong>partial_n:</strong> select randomly partial quantity of data at each epoch. Used to reduce the training size (for example for testing purposes). int or float can be used.</li>
<li><strong>inplace:</strong> indicates whether tfms are applied during instantiation or on-the-fly. Default=True, which means that tfms will be applied during instantiation. This results in a faster training, but it can only be used when data fits in memory. Otherwise set it to False. </li>
<li><strong>shuffle_train:</strong> indicates whether to shuffle the training set every time the dataloader is fully read/iterated or not. This doesn't have an impact on the validation set which is never shuffled. Default=True.</li>
<li><strong>drop_last:</strong> if True the last incomplete training batch is dropped (thus ensuring training batches of equal size). This doesn't have an impact on the validation set where samples are never dropped. Default=True.</li>
<li><strong>num_workers:</strong> num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default=0. </li>
<li><strong>do_setup:</strong> ndicates if the Pipeline.setup method should be called during initialization. Default=True.</li>
<li><strong>device:</strong> Defaults to default_device() which is CUDA by default. You can specify device as `torch.device('cpu').</li>
<li><strong>verbose:</strong> controls the verbosity when fitting and predicting.</li>
<li><strong>exclude_head:</strong> indicates whether the head of the pretrained model needs to be removed or not. Default=True.</li>
<li><strong>cut:</strong> indicates the position where the pretrained model head needs to be cut. Defaults=-1.</li>
<li><strong>init:</strong> allows you to set to None (no initialization applied), set to True (in which case nn.init.kaiming<em>normal</em> will be applied) or pass an initialization. Default=None.</li>
<li><strong>splitter:</strong> To do transfer learning, you need to pass a splitter to Learner. This should be a function taking the model and returning a collection of parameter groups, e.g. a list of list of parameters. Default=trainable_params. If the model has a backbone and a head, it will then be split in those 2 groups.</li>
<li><strong>path</strong> and <strong>model_dir:</strong> are used to save and/or load models. Often path will be inferred from dls, but you can override it or pass a Path object to model_dir.</li>
<li><strong>wd_bn_bias:</strong> controls if weight decay is applied to BatchNorm layers and bias. Default=False.
train_bn=True</li>
<li><strong>moms:</strong> the default momentums used in Learner.fit_one_cycle. Default=(0.95, 0.85, 0.95).</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TSClassifier" class="doc_header"><code>class</code> <code>TSClassifier</code><a href="https://github.com/timeseriesAI/tsai/tree/main/tsai/tslearner.py#L14" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TSClassifier</code>(<strong><code>X</code></strong>, <strong><code>y</code></strong>=<em><code>None</code></em>, <strong><code>splits</code></strong>=<em><code>None</code></em>, <strong><code>tfms</code></strong>=<em><code>None</code></em>, <strong><code>inplace</code></strong>=<em><code>True</code></em>, <strong><code>sel_vars</code></strong>=<em><code>None</code></em>, <strong><code>sel_steps</code></strong>=<em><code>None</code></em>, <strong><code>weights</code></strong>=<em><code>None</code></em>, <strong><code>partial_n</code></strong>=<em><code>None</code></em>, <strong><code>bs</code></strong>=<em><code>[64, 128]</code></em>, <strong><code>batch_size</code></strong>=<em><code>None</code></em>, <strong><code>batch_tfms</code></strong>=<em><code>None</code></em>, <strong><code>shuffle_train</code></strong>=<em><code>True</code></em>, <strong><code>drop_last</code></strong>=<em><code>True</code></em>, <strong><code>num_workers</code></strong>=<em><code>0</code></em>, <strong><code>do_setup</code></strong>=<em><code>True</code></em>, <strong><code>device</code></strong>=<em><code>None</code></em>, <strong><code>arch</code></strong>=<em><code>None</code></em>, <strong><code>arch_config</code></strong>=<em><code>{}</code></em>, <strong><code>pretrained</code></strong>=<em><code>False</code></em>, <strong><code>weights_path</code></strong>=<em><code>None</code></em>, <strong><code>exclude_head</code></strong>=<em><code>True</code></em>, <strong><code>cut</code></strong>=<em><code>-1</code></em>, <strong><code>init</code></strong>=<em><code>None</code></em>, <strong><code>loss_func</code></strong>=<em><code>None</code></em>, <strong><code>opt_func</code></strong>=<em><code>Adam</code></em>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>metrics</code></strong>=<em><code>accuracy</code></em>, <strong><code>cbs</code></strong>=<em><code>None</code></em>, <strong><code>wd</code></strong>=<em><code>None</code></em>, <strong><code>wd_bn_bias</code></strong>=<em><code>False</code></em>, <strong><code>train_bn</code></strong>=<em><code>True</code></em>, <strong><code>moms</code></strong>=<em><code>(0.95, 0.85, 0.95)</code></em>, <strong><code>path</code></strong>=<em><code>'.'</code></em>, <strong><code>model_dir</code></strong>=<em><code>'models'</code></em>, <strong><code>splitter</code></strong>=<em><code>trainable_params</code></em>, <strong><code>verbose</code></strong>=<em><code>False</code></em>) :: <code>Learner</code></p>
</blockquote>
<p>Group together a <code>model</code>, some <code>dls</code> and a <code>loss_func</code> to handle training</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tsai.models.InceptionTimePlus</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">splits</span> <span class="o">=</span> <span class="n">get_classification_data</span><span class="p">(</span><span class="s1">&#39;OliveOil&#39;</span><span class="p">,</span> <span class="n">split_data</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">batch_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">TSStandardize</span><span class="p">(</span><span class="n">by_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">TSClassifier</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">,</span> <span class="n">batch_tfms</span><span class="o">=</span><span class="n">batch_tfms</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">arch</span><span class="o">=</span><span class="n">InceptionTimePlus</span><span class="p">,</span> <span class="n">arch_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">fc_dropout</span><span class="o">=.</span><span class="mi">5</span><span class="p">))</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.397710</td>
      <td>1.367821</td>
      <td>0.400000</td>
      <td>00:09</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/nacho/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)
  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TSRegressor-API">TSRegressor API<a class="anchor-link" href="#TSRegressor-API"> </a></h2><hr>
<p><strong>Commonly used arguments:</strong></p>
<ul>
<li><strong>X:</strong> array-like of shape (n_samples, n_steps) or (n_samples, n_features, n_steps) with the input time series samples. Internally, they will be converted to torch tensors.</li>
<li><strong>y:</strong> array-like of shape (n_samples), (n_samples, n_outputs) or (n_samples, n_features, n_outputs) with the target. Internally, they will be converted to torch tensors. Default=None. None is used for unlabeled datasets. </li>
<li><strong>splits:</strong> lists of indices used to split data between train and validation. Default=None. If no splits are passed, data will be split 80:20 between train and test without shuffling.</li>
<li><strong>tfms:</strong> item transforms that will be applied to each sample individually. Default=<code>[None, TSRegression()]</code> which is commonly used in most single label datasets. </li>
<li><strong>batch_tfms:</strong> transforms applied to each batch. Default=None. </li>
<li><strong>bs:</strong> batch size (if batch_size is provided then batch_size will override bs). An int or a list of ints can be passed. Default=<code>[64, 128]</code>. If a list of ints, the first one will be used for training, and the second for the valid (batch size can be larger as it doesn't require backpropagation which consumes more memory). </li>
<li><strong>arch:</strong> indicates which architecture will be used. Default: InceptionTimePlus.</li>
<li><strong>arch_config:</strong> keyword arguments passed to the selected architecture. Default={}.</li>
<li><strong>pretrained:</strong> indicates if pretrained model weights will be used. Default=False.</li>
<li><strong>weights_path:</strong> indicates the path to the pretrained weights in case they are used.</li>
<li><strong>loss_func:</strong> allows you to pass any loss function. Default=None (in which case CrossEntropyLossFlat() is applied).</li>
<li><strong>opt_func:</strong> allows you to pass an optimizer. Default=Adam.</li>
<li><strong>lr:</strong> learning rate. Default=0.001.</li>
<li><strong>metrics:</strong> list of metrics passed to the Learner. Default=None.</li>
<li><strong>cbs:</strong> list of callbacks passed to the Learner. Default=None.</li>
<li><strong>wd:</strong> is the default weight decay used when training the model. Default=None.</li>
</ul>
<p><strong>Less frequently used arguments:</strong></p>
<ul>
<li><strong>sel_vars:</strong> used to select which of the features in multivariate datasets are used. Default=None means all features are used. If necessary a list-like of indices can be used (eg.<code>[0,3,5]</code>).</li>
<li><strong>sel_steps:</strong> used to select the steps used. Default=None means all steps are used. If necessary a list-like of indices can be used (eg. <code>slice(-50, None)</code> will select the last 50 steps from each time series).</li>
<li><strong>weights:</strong> indicates a sample weight per instance. Used to pass pass a probability to the train dataloader sampler. Samples with more weight will be selected more often during training. </li>
<li><strong>partial_n:</strong> select randomly partial quantity of data at each epoch. Used to reduce the training size (for example for testing purposes). int or float can be used.</li>
<li><strong>inplace:</strong> indicates whether tfms are applied during instantiation or on-the-fly. Default=True, which means that tfms will be applied during instantiation. This results in a faster training, but it can only be used when data fits in memory. Otherwise set it to False. </li>
<li><strong>shuffle_train:</strong> indicates whether to shuffle the training set every time the dataloader is fully read/iterated or not. This doesn't have an impact on the validation set which is never shuffled. Default=True.</li>
<li><strong>drop_last:</strong> if True the last incomplete training batch is dropped (thus ensuring training batches of equal size). This doesn't have an impact on the validation set where samples are never dropped. Default=True.</li>
<li><strong>num_workers:</strong> num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default=0. </li>
<li><strong>do_setup:</strong> ndicates if the Pipeline.setup method should be called during initialization. Default=True.</li>
<li><strong>device:</strong> Defaults to default_device() which is CUDA by default. You can specify device as `torch.device('cpu').</li>
<li><strong>verbose:</strong> controls the verbosity when fitting and predicting.</li>
<li><strong>exclude_head:</strong> indicates whether the head of the pretrained model needs to be removed or not. Default=True.</li>
<li><strong>cut:</strong> indicates the position where the pretrained model head needs to be cut. Defaults=-1.</li>
<li><strong>init:</strong> allows you to set to None (no initialization applied), set to True (in which case nn.init.kaiming<em>normal</em> will be applied) or pass an initialization. Default=None.</li>
<li><strong>splitter:</strong> To do transfer learning, you need to pass a splitter to Learner. This should be a function taking the model and returning a collection of parameter groups, e.g. a list of list of parameters. Default=trainable_params. If the model has a backbone and a head, it will then be split in those 2 groups.</li>
<li><strong>path</strong> and <strong>model_dir:</strong> are used to save and/or load models. Often path will be inferred from dls, but you can override it or pass a Path object to model_dir.</li>
<li><strong>wd_bn_bias:</strong> controls if weight decay is applied to BatchNorm layers and bias. Default=False.
train_bn=True</li>
<li><strong>moms:</strong> the default momentums used in Learner.fit_one_cycle. Default=(0.95, 0.85, 0.95).</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TSRegressor" class="doc_header"><code>class</code> <code>TSRegressor</code><a href="https://github.com/timeseriesAI/tsai/tree/main/tsai/tslearner.py#L68" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TSRegressor</code>(<strong><code>X</code></strong>, <strong><code>y</code></strong>=<em><code>None</code></em>, <strong><code>splits</code></strong>=<em><code>None</code></em>, <strong><code>tfms</code></strong>=<em><code>None</code></em>, <strong><code>inplace</code></strong>=<em><code>True</code></em>, <strong><code>sel_vars</code></strong>=<em><code>None</code></em>, <strong><code>sel_steps</code></strong>=<em><code>None</code></em>, <strong><code>weights</code></strong>=<em><code>None</code></em>, <strong><code>partial_n</code></strong>=<em><code>None</code></em>, <strong><code>bs</code></strong>=<em><code>[64, 128]</code></em>, <strong><code>batch_size</code></strong>=<em><code>None</code></em>, <strong><code>batch_tfms</code></strong>=<em><code>None</code></em>, <strong><code>shuffle_train</code></strong>=<em><code>True</code></em>, <strong><code>drop_last</code></strong>=<em><code>True</code></em>, <strong><code>num_workers</code></strong>=<em><code>0</code></em>, <strong><code>do_setup</code></strong>=<em><code>True</code></em>, <strong><code>device</code></strong>=<em><code>None</code></em>, <strong><code>arch</code></strong>=<em><code>None</code></em>, <strong><code>arch_config</code></strong>=<em><code>{}</code></em>, <strong><code>pretrained</code></strong>=<em><code>False</code></em>, <strong><code>weights_path</code></strong>=<em><code>None</code></em>, <strong><code>exclude_head</code></strong>=<em><code>True</code></em>, <strong><code>cut</code></strong>=<em><code>-1</code></em>, <strong><code>init</code></strong>=<em><code>None</code></em>, <strong><code>loss_func</code></strong>=<em><code>None</code></em>, <strong><code>opt_func</code></strong>=<em><code>Adam</code></em>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>metrics</code></strong>=<em><code>None</code></em>, <strong><code>cbs</code></strong>=<em><code>None</code></em>, <strong><code>wd</code></strong>=<em><code>None</code></em>, <strong><code>wd_bn_bias</code></strong>=<em><code>False</code></em>, <strong><code>train_bn</code></strong>=<em><code>True</code></em>, <strong><code>moms</code></strong>=<em><code>(0.95, 0.85, 0.95)</code></em>, <strong><code>path</code></strong>=<em><code>'.'</code></em>, <strong><code>model_dir</code></strong>=<em><code>'models'</code></em>, <strong><code>splitter</code></strong>=<em><code>trainable_params</code></em>, <strong><code>verbose</code></strong>=<em><code>False</code></em>) :: <code>Learner</code></p>
</blockquote>
<p>Group together a <code>model</code>, some <code>dls</code> and a <code>loss_func</code> to handle training</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tsai.models.TST</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">splits</span> <span class="o">=</span> <span class="n">get_regression_data</span><span class="p">(</span><span class="s1">&#39;AppliancesEnergy&#39;</span><span class="p">,</span> <span class="n">split_data</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">batch_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">TSStandardize</span><span class="p">()]</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">TSRegressor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">,</span> <span class="n">batch_tfms</span><span class="o">=</span><span class="n">batch_tfms</span><span class="p">,</span> <span class="n">arch</span><span class="o">=</span><span class="n">TST</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">mae</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>mae</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>217.123535</td>
      <td>199.025177</td>
      <td>13.682029</td>
      <td>00:11</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TSForecaster-API">TSForecaster API<a class="anchor-link" href="#TSForecaster-API"> </a></h2><hr>
<p><strong>Commonly used arguments:</strong></p>
<ul>
<li><strong>X:</strong> array-like of shape (n_samples, n_steps) or (n_samples, n_features, n_steps) with the input time series samples. Internally, they will be converted to torch tensors.</li>
<li><strong>y:</strong> array-like of shape (n_samples), (n_samples, n_outputs) or (n_samples, n_features, n_outputs) with the target. Internally, they will be converted to torch tensors. Default=None. None is used for unlabeled datasets. </li>
<li><strong>splits:</strong> lists of indices used to split data between train and validation. Default=None. If no splits are passed, data will be split 80:20 between train and test without shuffling.</li>
<li><strong>tfms:</strong> item transforms that will be applied to each sample individually. Default=<code>[None, TSForecasting()]</code> which is commonly used in most single label datasets. </li>
<li><strong>batch_tfms:</strong> transforms applied to each batch. Default=None. </li>
<li><strong>bs:</strong> batch size (if batch_size is provided then batch_size will override bs). An int or a list of ints can be passed. Default=<code>[64, 128]</code>. If a list of ints, the first one will be used for training, and the second for the valid (batch size can be larger as it doesn't require backpropagation which consumes more memory). </li>
<li><strong>arch:</strong> indicates which architecture will be used. Default: InceptionTimePlus.</li>
<li><strong>arch_config:</strong> keyword arguments passed to the selected architecture. Default={}.</li>
<li><strong>pretrained:</strong> indicates if pretrained model weights will be used. Default=False.</li>
<li><strong>weights_path:</strong> indicates the path to the pretrained weights in case they are used.</li>
<li><strong>loss_func:</strong> allows you to pass any loss function. Default=None (in which case CrossEntropyLossFlat() is applied).</li>
<li><strong>opt_func:</strong> allows you to pass an optimizer. Default=Adam.</li>
<li><strong>lr:</strong> learning rate. Default=0.001.</li>
<li><strong>metrics:</strong> list of metrics passed to the Learner. Default=None.</li>
<li><strong>cbs:</strong> list of callbacks passed to the Learner. Default=None.</li>
<li><strong>wd:</strong> is the default weight decay used when training the model. Default=None.</li>
</ul>
<p><strong>Less frequently used arguments:</strong></p>
<ul>
<li><strong>sel_vars:</strong> used to select which of the features in multivariate datasets are used. Default=None means all features are used. If necessary a list-like of indices can be used (eg.<code>[0,3,5]</code>).</li>
<li><strong>sel_steps:</strong> used to select the steps used. Default=None means all steps are used. If necessary a list-like of indices can be used (eg. <code>slice(-50, None)</code> will select the last 50 steps from each time series).</li>
<li><strong>weights:</strong> indicates a sample weight per instance. Used to pass pass a probability to the train dataloader sampler. Samples with more weight will be selected more often during training. </li>
<li><strong>partial_n:</strong> select randomly partial quantity of data at each epoch. Used to reduce the training size (for example for testing purposes). int or float can be used.</li>
<li><strong>inplace:</strong> indicates whether tfms are applied during instantiation or on-the-fly. Default=True, which means that tfms will be applied during instantiation. This results in a faster training, but it can only be used when data fits in memory. Otherwise set it to False. </li>
<li><strong>shuffle_train:</strong> indicates whether to shuffle the training set every time the dataloader is fully read/iterated or not. This doesn't have an impact on the validation set which is never shuffled. Default=True.</li>
<li><strong>drop_last:</strong> if True the last incomplete training batch is dropped (thus ensuring training batches of equal size). This doesn't have an impact on the validation set where samples are never dropped. Default=True.</li>
<li><strong>num_workers:</strong> num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default=None. </li>
<li><strong>do_setup:</strong> ndicates if the Pipeline.setup method should be called during initialization. Default=True.</li>
<li><strong>device:</strong> Defaults to default_device() which is CUDA by default. You can specify device as `torch.device('cpu').</li>
<li><strong>verbose:</strong> controls the verbosity when fitting and predicting.</li>
<li><strong>exclude_head:</strong> indicates whether the head of the pretrained model needs to be removed or not. Default=True.</li>
<li><strong>cut:</strong> indicates the position where the pretrained model head needs to be cut. Defaults=-1.</li>
<li><strong>init:</strong> allows you to set to None (no initialization applied), set to True (in which case nn.init.kaiming<em>normal</em> will be applied) or pass an initialization. Default=None.</li>
<li><strong>splitter:</strong> To do transfer learning, you need to pass a splitter to Learner. This should be a function taking the model and returning a collection of parameter groups, e.g. a list of list of parameters. Default=trainable_params. If the model has a backbone and a head, it will then be split in those 2 groups.</li>
<li><strong>path</strong> and <strong>model_dir:</strong> are used to save and/or load models. Often path will be inferred from dls, but you can override it or pass a Path object to model_dir.</li>
<li><strong>wd_bn_bias:</strong> controls if weight decay is applied to BatchNorm layers and bias. Default=False.
train_bn=True</li>
<li><strong>moms:</strong> the default momentums used in Learner.fit_one_cycle. Default=(0.95, 0.85, 0.95).</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TSForecaster" class="doc_header"><code>class</code> <code>TSForecaster</code><a href="https://github.com/timeseriesAI/tsai/tree/main/tsai/tslearner.py#L121" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TSForecaster</code>(<strong><code>X</code></strong>, <strong><code>y</code></strong>=<em><code>None</code></em>, <strong><code>splits</code></strong>=<em><code>None</code></em>, <strong><code>tfms</code></strong>=<em><code>None</code></em>, <strong><code>inplace</code></strong>=<em><code>True</code></em>, <strong><code>sel_vars</code></strong>=<em><code>None</code></em>, <strong><code>sel_steps</code></strong>=<em><code>None</code></em>, <strong><code>weights</code></strong>=<em><code>None</code></em>, <strong><code>partial_n</code></strong>=<em><code>None</code></em>, <strong><code>bs</code></strong>=<em><code>[64, 128]</code></em>, <strong><code>batch_size</code></strong>=<em><code>None</code></em>, <strong><code>batch_tfms</code></strong>=<em><code>None</code></em>, <strong><code>shuffle_train</code></strong>=<em><code>True</code></em>, <strong><code>drop_last</code></strong>=<em><code>True</code></em>, <strong><code>num_workers</code></strong>=<em><code>0</code></em>, <strong><code>do_setup</code></strong>=<em><code>True</code></em>, <strong><code>device</code></strong>=<em><code>None</code></em>, <strong><code>arch</code></strong>=<em><code>None</code></em>, <strong><code>arch_config</code></strong>=<em><code>{}</code></em>, <strong><code>pretrained</code></strong>=<em><code>False</code></em>, <strong><code>weights_path</code></strong>=<em><code>None</code></em>, <strong><code>exclude_head</code></strong>=<em><code>True</code></em>, <strong><code>cut</code></strong>=<em><code>-1</code></em>, <strong><code>init</code></strong>=<em><code>None</code></em>, <strong><code>loss_func</code></strong>=<em><code>None</code></em>, <strong><code>opt_func</code></strong>=<em><code>Adam</code></em>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>metrics</code></strong>=<em><code>None</code></em>, <strong><code>cbs</code></strong>=<em><code>None</code></em>, <strong><code>wd</code></strong>=<em><code>None</code></em>, <strong><code>wd_bn_bias</code></strong>=<em><code>False</code></em>, <strong><code>train_bn</code></strong>=<em><code>True</code></em>, <strong><code>moms</code></strong>=<em><code>(0.95, 0.85, 0.95)</code></em>, <strong><code>path</code></strong>=<em><code>'.'</code></em>, <strong><code>model_dir</code></strong>=<em><code>'models'</code></em>, <strong><code>splitter</code></strong>=<em><code>trainable_params</code></em>, <strong><code>verbose</code></strong>=<em><code>False</code></em>) :: <code>Learner</code></p>
</blockquote>
<p>Group together a <code>model</code>, some <code>dls</code> and a <code>loss_func</code> to handle training</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tsai.models.TSTPlus</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">ts</span> <span class="o">=</span> <span class="n">get_forecasting_time_series</span><span class="p">(</span><span class="s1">&#39;Sunspots&#39;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">ts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="c1"># this is for testing purposes, as sometimes the url is down, and data cannot be downloaded, failing the test.</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">SlidingWindowSplitter</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="n">horizon</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">ts</span><span class="p">)</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="n">TSSplitter</span><span class="p">(</span><span class="mi">235</span><span class="p">)(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">batch_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">TSStandardize</span><span class="p">(</span><span class="n">by_var</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>
    <span class="n">learn</span> <span class="o">=</span> <span class="n">TSForecaster</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">,</span> <span class="n">batch_tfms</span><span class="o">=</span><span class="n">batch_tfms</span><span class="p">,</span> <span class="n">arch</span><span class="o">=</span><span class="n">TST</span><span class="p">,</span> <span class="n">arch_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">fc_dropout</span><span class="o">=.</span><span class="mi">5</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">mae</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">partial_n</span><span class="o">=.</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Dataset: Sunspots
downloading data...
...data downloaded. Path = data/forecasting/Sunspots.csv
</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>mae</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>12002.387695</td>
      <td>8266.375000</td>
      <td>88.514038</td>
      <td>00:06</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

