---

title: TSiT & InceptionTSiT


keywords: fastai
sidebar: home_sidebar

summary: "These are PyTorch implementations created by Ignacio Oguiza (timeseriesAI@gmail.com) based on ViT (Vision Transformer)"
description: "These are PyTorch implementations created by Ignacio Oguiza (timeseriesAI@gmail.com) based on ViT (Vision Transformer)"
nb_path: "nbs/124_models.TSiTPlus.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/124_models.TSiTPlus.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TSiTPlus" class="doc_header"><code>class</code> <code>TSiTPlus</code><a href="https://github.com/timeseriesAI/tsai/tree/main/tsai/models/TSiTPlus.py#L100" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TSiTPlus</code>(<strong><code>c_in</code></strong>:<code>int</code>, <strong><code>c_out</code></strong>:<code>int</code>, <strong><code>seq_len</code></strong>:<code>int</code>, <strong><code>n_layers</code></strong>:<code>int</code>=<em><code>6</code></em>, <strong><code>d_model</code></strong>:<code>int</code>=<em><code>128</code></em>, <strong><code>n_heads</code></strong>:<code>int</code>=<em><code>16</code></em>, <strong><code>d_head</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>act</code></strong>:<code>str</code>=<em><code>'reglu'</code></em>, <strong><code>d_ff</code></strong>:<code>int</code>=<em><code>256</code></em>, <strong><code>pos_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>attn_drop_rate</code></strong>:<code>float</code>=<em><code>0</code></em>, <strong><code>mlp_drop_rate</code></strong>:<code>float</code>=<em><code>0</code></em>, <strong><code>drop_path_rate</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>mlp_ratio</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>qkv_bias</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>pre_norm</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>use_token</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>fc_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>bn</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>y_range</code></strong>:<code>Optional</code>[<code>tuple</code>]=<em><code>None</code></em>, <strong><code>ks</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>maxpool</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>preprocessor</code></strong>:<code>Optional</code>[<code>Callable</code>]=<em><code>None</code></em>, <strong><code>custom_head</code></strong>:<code>Optional</code>[<code>Callable</code>]=<em><code>None</code></em>, <strong><code>verbose</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <a href="/models.TabFusionTransformer.html#Sequential"><code>Sequential</code></a></p>
</blockquote>
<p>Time series transformer model based on ViT (Vision Transformer):</p>
<p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... &amp; Houlsby, N. (2020).
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.</p>
<p>This implementation is a modified version of Vision Transformer that is part of the grat timm library
(<a href="https://github.com/rwightman/pytorch-image-models/blob/72b227dcf57c0c62291673b96bdc06576bb90457/timm/models/vision_transformer.py">https://github.com/rwightman/pytorch-image-models/blob/72b227dcf57c0c62291673b96bdc06576bb90457/timm/models/vision_transformer.py</a>)</p>
<h1 id="Args:">Args:<a class="anchor-link" href="#Args:"> </a></h1><p>c_in:                   the number of features (aka variables, dimensions, channels) in the time series dataset.
c_out:                  the number of target classes.
seq_len:                number of time steps in the time series.
n_layers:               number of layers (or blocks) in the encoder. Default: 3 (range(1-4))
d_model:                total dimension of the model (number of features created by the model). Default: 128 (range(64-512))
n_heads:                parallel attention heads. Default:16 (range(8-16)).
d_head:                 size of the learned linear projection of queries, keys and values in the MHA. Usual values: 16-512.
                        Default: None -&gt; (d_model/n_heads) = 32.
act:                    the activation function of intermediate layer, relu, gelu, geglu, reglu.
d_ff:                   the dimension of the feedforward network model. Default: 512 (range(256-512))
pos_dropout:            dropout applied to to the embedded sequence steps after position embeddings have been added.
attn_drop_rate (float): dropout rate applied to the attention layer
mlp_drop_rate (float):  dropout rate applied to the mlp layer
drop_path_rate:         dropout applied to the output of MultheadAttention and PositionwiseFeedForward layers.
mlp_ratio:              ratio of mlp hidden dim to embedding dim.
qkv_bias:               determines whether bias is applied to the Linear projections of queries, keys and values in the MultiheadAttention
pre_norm:               if True normalization will be applied as the first step in the sublayers. Defaults to False.
use_token:              if True, the output will come from the transformed token. Otherwise a pooling layer will be applied.
fc_dropout:             dropout applied to the final fully connected layer.
bn:                     indicates if batchnorm will be applied to the head.
y_range:                range of possible y values (used in regression tasks).
ks:                     (Optional) kernel sizes that will be applied to a hybrid embedding.
maxpool:                If true and kernel sizes are passed, maxpool will also be added to the hybrid embedding.
preprocessor:           an optional callable (nn.Conv1d with dilation &gt; 1 or stride &gt; 1 for example) that will be used to preprocess the time series before
                        the embedding step. It is useful to extract features or resample the time series.
custom_head:            custom head that will be applied to the network. It must contain all kwargs (pass a partial function)</p>
<p>Input shape:
    x: bs (batch size) x nvars (aka features, variables, dimensions, channels) x seq_len (aka time steps)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">nvars</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">c_out</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">xb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">nvars</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TSiTPlus</span><span class="p">(</span><span class="n">nvars</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c_out</span><span class="p">))</span>
<span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>TSiTPlus(
  (backbone): _TSiTBackbone(
    (preprocessor): Identity()
    (to_embedding): Sequential(
      (0): Conv1d(4, 128, kernel_size=(1,), stride=(1,))
      (1): Transpose(1, 2)
    )
    (pos_dropout): Dropout(p=0.0, inplace=False)
    (encoder): _TSiTEncoder(
      (layers): ModuleList(
        (0): ModuleList(
          (0): MultiheadAttention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention()
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): PositionwiseFeedForward(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): ReGLU()
            (2): Dropout(p=0, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): Dropout(p=0, inplace=False)
          )
          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (4): Identity()
        )
        (1): ModuleList(
          (0): MultiheadAttention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention()
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): PositionwiseFeedForward(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): ReGLU()
            (2): Dropout(p=0, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): Dropout(p=0, inplace=False)
          )
          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (4): Identity()
        )
        (2): ModuleList(
          (0): MultiheadAttention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention()
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): PositionwiseFeedForward(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): ReGLU()
            (2): Dropout(p=0, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): Dropout(p=0, inplace=False)
          )
          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (4): Identity()
        )
        (3): ModuleList(
          (0): MultiheadAttention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention()
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): PositionwiseFeedForward(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): ReGLU()
            (2): Dropout(p=0, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): Dropout(p=0, inplace=False)
          )
          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (4): Identity()
        )
        (4): ModuleList(
          (0): MultiheadAttention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention()
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): PositionwiseFeedForward(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): ReGLU()
            (2): Dropout(p=0, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): Dropout(p=0, inplace=False)
          )
          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (4): Identity()
        )
        (5): ModuleList(
          (0): MultiheadAttention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention()
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): PositionwiseFeedForward(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): ReGLU()
            (2): Dropout(p=0, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): Dropout(p=0, inplace=False)
          )
          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (4): Identity()
        )
      )
      (norm): Identity()
    )
  )
  (head): Sequential(
    (0): TokenLayer()
    (1): LinBnDrop(
      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=128, out_features=2, bias=False)
    )
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">nvars</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">c_out</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">xb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">nvars</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">InceptionTSiTPlus</span><span class="p">(</span><span class="n">nvars</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c_out</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/nacho/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)
  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Subsampling">Subsampling<a class="anchor-link" href="#Subsampling"> </a></h3><p>It's a known fact that transformers cannot be directly applied to long sequences. To avoid this, we have included a way to subsample the sequence to generate a more manageable input.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tsai.data.validation</span> <span class="kn">import</span> <span class="n">get_splits</span>
<span class="kn">from</span> <span class="nn">tsai.data.core</span> <span class="kn">import</span> <span class="n">get_ts_dls</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5000</span><span class="p">))</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">splits</span> <span class="o">=</span> <span class="n">get_splits</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">get_ts_dls</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">)</span>
<span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="n">xb</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>TSTensor(samples:8, vars:3, len:5000, device=cpu)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you try to use TSiTPlus, it's likely you'll get an 'out-of-memory' error.</p>
<p>To avoid this you can subsample the sequence reducing the input's length. This can be done in multiple ways. Here are a few examples:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">Conv1d</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ks</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">())</span>
<span class="n">preprocessor</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([8, 3, 100])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">Conv1d</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ks</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">())</span>
<span class="n">preprocessor</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([8, 2, 100])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Pad1d</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">50</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">())</span>
<span class="n">preprocessor</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([8, 3, 100])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Pad1d</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">50</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">())</span>
<span class="n">preprocessor</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([8, 3, 100])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Once you decide what type of transform you want to apply, you just need to pass the layer as the preprocessor attribute:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">nvars</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">c_out</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">xb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">nvars</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">Conv1d</span><span class="p">,</span> <span class="n">ks</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TSiTPlus</span><span class="p">(</span><span class="n">nvars</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">preprocessor</span><span class="o">=</span><span class="n">preprocessor</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c_out</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

